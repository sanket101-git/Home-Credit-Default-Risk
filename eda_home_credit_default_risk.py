# -*- coding: utf-8 -*-
"""EDA_Home_Credit_Default_Risk.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QKrJ6R8XyivsVCSEg26MnQZ7ND5Hx4f_

# Home_Credit_Default_Risk_EDA Assignment

## Table of contents:-
0. <a href="#Task-0">Introduction</a>
1. <a href="#Task-0">Task 0:- Library and Data import</a>
2. <a href="#Task-1">Task 1:- Explore the target variable</a>
3. <a href="#Task-2">Task 2:- Explore the relationship between target and predictors</a>
4. <a href="#Task-3">Task 3:- Data exploration Details</a>
5. <a href="#Task-4">Task 4:- Explore the scope of missing data in application</a>
6. <a href="#Task-5">Task 5:- Handleing outliers and columns with near-zero or zero variance </a>
7. <a href="#Task-6">Task 6:- Transformation of Input Data</a>
8. <a href="#Task-7">Task 7:- Join application_{train|test}.csv with transactional data</a>
9. <a href="#Task-8">Task 8:- Explore the joined transactional data</a>
10. <a href="#Task-8">Result</a>

## Introduction

Home Credit, an international consumer finance provider operating in nine countries, specializes in offering point-of-sale loans, cash loans, and revolving loans to underserved borrowers. These borrowers typically have regular income from employment or businesses but struggle to access credit due to limited or nonexistent credit histories. Home Credit believes that everyone deserves the opportunity to fulfill their financial aspirations, regardless of their credit history.

The primary objective of the project is to harness the wealth of borrower behavioral data to develop predictive models. These models will empower Home Credit to effectively assess the risk associated with individual clients and determine the appropriate amount of credit to extend, even for those with minimal credit history. By leveraging these predictive tools, Home Credit aims to provide financial assistance to customers while upholding responsible lending practices, thereby enabling them to realize their dreams and aspirations.

## Task 0

### Libraries and Data Import
"""

import numpy as np
import pandas as pd
import seaborn as sns
from matplotlib import pyplot as plt
from sklearn.ensemble import RandomForestClassifier

import warnings
warnings.filterwarnings("ignore")

home_credit_train = pd.read_csv("/content/application_train.csv")

home_credit_test = pd.read_csv("/content/application_test.csv")

home_credit_train.describe()

home_credit_test.describe()

"""## Task 1

### Explore the target variable
"""

home_credit_train['TARGET'].value_counts()*100/len(home_credit_train)

plt.figure(figsize=(8, 5))
sns.countplot(x='TARGET', data=home_credit_train)
plt.title('Distribution of TARGET Variable')
plt.show()

"""The data is unbalanced with respect to the target as the distribution of the target variable is about 92% for 0(Non-defaulters) and only 8% for 1(defaulters)"""

accuracy = home_credit_train["TARGET"].value_counts().max()*100 / len(home_credit_train)
print("Majority class classifier accuracy:", accuracy)

# The accuracy be for a simple model consisting in a majority class classifier would be 91.92%

"""## Task 2

### Explore the relationship between target and predictors, looking for potentially strong predictors that could be included later in a model.
"""

numeric_predictors = home_credit_train.select_dtypes(include=['int64', 'float64']).columns.tolist()
# Visualize relationship for numerical predictors
# for predictor in numeric_predictors:
#     sns.boxplot(x='TARGET', y=predictor, data=home_credit_train)
#     plt.title(f'Relationship between {predictor} and TARGET')
#     plt.figure(figsize=(10, 10))
#     plt.show()

correlation_matrix = home_credit_train.select_dtypes(include=['int64', 'float64']).corr()
strong_correlations = correlation_matrix['TARGET'].sort_values(ascending=False)
print("positive Correlations with TARGET:")
print(strong_correlations.head(6))
print("negative Correlations with TARGET:")
print(strong_correlations.tail(6))
#print(strong_correlations[strong_correlations<0].tail(10))

# These are the top 6 strong numeric predictors that could be included later in a model.

# Selecting a few numerical variables for the correlation matrix
numerical_features = ['TARGET','DAYS_BIRTH', 'REGION_RATING_CLIENT', 'FLOORSMAX_MEDI','EXT_SOURCE_3']
correlation_matrix1 = home_credit_train[numerical_features].corr()

# Plotting the heatmap
sns.heatmap(correlation_matrix1, annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap of Selected Numerical Features')
plt.show()

"""Correlation Heatmap of Selected Numerical Features"""

categorical_predictors = home_credit_train.select_dtypes(include=['object']).columns.tolist()

# Visualize relationship for categorical predictors
for predictor in categorical_predictors:
    sns.countplot(x=predictor, hue='TARGET', data=home_credit_train)
    plt.title(f'Relationship between {predictor} and TARGET')
    plt.xticks(rotation=45)
    plt.show()

"""Relationship between predictor and TARGET

## Task 3

### The skimr package in R has some great data exploration tools, and the janitor package has utilities that will simplify data cleaning.
"""

# We can accomplish similar tasks using libraries such as pandas, seaborn,matplotlib and scikit-learn.

"""## Task 4

###  Explore the scope of missing data in application_{train|test}.csv and come up with possible solutions. Remove rows?  Remove columns?  Impute?
"""

# Check for missing values
train_missing = home_credit_train.isnull().sum()
test_missing = home_credit_test.isnull().sum()

pd.options.display.max_rows = 300
train_missing

len(home_credit_train)

train_missing_percent = (train_missing / len(home_credit_train)) * 100

missing_data = pd.DataFrame({
    'Train Missing': train_missing,
    'Train_Missing_percentage': train_missing_percent})
print(missing_data)

a=missing_data[missing_data['Train_Missing_percentage']>50].index.tolist()

home_credit_train.drop(a,axis=1,inplace=True)

home_credit_train.shape

# There are many columns which consist of null data.
# If the columns have missing values more than 50% then they are droped and remaining columns with missing data less than 50% can be imputed with filling the mean values or the most frequently used values in the respective columns.

train_missing1 = home_credit_train.isnull().sum()
train_missing1

n_columns=home_credit_train.select_dtypes(include=['int64', 'float64']).columns.tolist()
for i in n_columns:
    if home_credit_train[i].isnull().sum()>0:
        home_credit_train[i].fillna(home_credit_train[i].mean(),inplace=True)
    else:
        home_credit_train[i].fillna(value=home_credit_train[i].mode()[0],inplace=True)

home_credit_train.info()

"""These are the two main tables provided and contains train set (contains TARGET) and test set. Organized by SK_ID_CURR
Train Set:
122 columns and 307511 rows
16 categorical 106 numeric
Test Set:
121 columns and 48744 rows
16 categorical 105 numeric

## Task 5

### Do the values make sense? Are there mistaken values that should be cleaned or imputed? (Note that outliers are not necessarily mistakes. Check APM for advice on how to handle outliers for a predictive project.) Are there columns with near-zero or zero variance?
"""

numeric_data = home_credit_train.select_dtypes(include=['int', 'float64'])

# Calculate variance for each numeric column
variance = numeric_data.var()

# Set a threshold for near-zero variance
threshold = 0.01

# Filter columns with near-zero or zero variance
near_zero_variance_cols = variance[variance <= threshold]

print(near_zero_variance_cols)

# These are the columns with near-zero variance

from scipy.stats import zscore

numerical_columns = numeric_data.select_dtypes(include=['float64', 'int64']).columns.tolist()
home_credit_train[numerical_columns].apply(zscore)

# There Zscore is applied for the standardization

"""## Task 6

### Will the input data need to be transformed in order to be used in a model?
"""

categorical_variables = home_credit_train.select_dtypes(include=['object', 'category']).columns.tolist()
print(categorical_variables)

home_credit_train[categorical_variables].head()

home_credit_train['NAME_INCOME_TYPE'].unique()

from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()

# Fit and transform the data to label encode
for col in categorical_variables:
    home_credit_train[col] = label_encoder.fit_transform(home_credit_train[col])

home_credit_train[categorical_variables].head()

"""Lable encoding is used to trasform the categorical variables into numerical format, making them suitable for machine learning algorithms.

## Task 7

### Join application_{train|test}.csv with transactional data in, for example, bureau.csv or previous_application.csv.
"""

df_bureau = pd.read_csv('/content/bureau.csv')
# Identifying the common columns
common_columns = list(set(df_bureau.columns) & set(home_credit_train.columns))
print("Common Columns:")
for column in common_columns:
    print(column)

# Joining the application_train dataset with bureau dataset
home_credit_train_merged = pd.merge(home_credit_train, df_bureau, on="SK_ID_CURR")
home_credit_train_merged.head()

"""## Task 8

### Explore the joined transactional data.  Do some of the added columns show promise in predicting default?
"""

home_credit_train_merged.info()

home_credit_train_merged.columns

new_col=['TARGET','SK_ID_BUREAU','DAYS_CREDIT', 'CREDIT_DAY_OVERDUE',
       'DAYS_CREDIT_ENDDATE', 'DAYS_ENDDATE_FACT', 'AMT_CREDIT_MAX_OVERDUE',
       'CNT_CREDIT_PROLONG', 'AMT_CREDIT_SUM', 'AMT_CREDIT_SUM_DEBT',
       'AMT_CREDIT_SUM_LIMIT', 'AMT_CREDIT_SUM_OVERDUE','DAYS_CREDIT_UPDATE', 'AMT_ANNUITY_y']

a=home_credit_train_merged[new_col].corr()
a['TARGET'].sort_values(ascending=False)

"""Columns like DAYS_CREDIT, DAYS_CREDIT_UPDATE ,DAYS_ENDDATE_FACT, DAYS_CREDIT_ENDDATE,AMT_CREDIT_SUM can impact on the predictions

## Results

There is a clear and substantial data imbalance when looking at the Target variable's distribution. They represent a minority class since just 8.07% of all loans in the dataset have been determined to have defaulted. The overwhelming class, however, consists of 91.9% of loans that are classified as non-defaulted. Target variables of 0 and 1 respectively represent non-defaulted loans and defaulted loans, respectively. The majority class may be favored in the evaluation findings if accuracy is the evaluation metric due to this imbalance. To get a more thorough evaluation of the model's performance, it is advised to use alternative evaluation metrics such the ROC-AUC Score, Log-Loss, F1-Score, or Confusion Matrix.
"""